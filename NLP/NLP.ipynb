{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "- Natural Language Processing (or NLP) is applying Machine Learning models to text and language.\n",
    "\n",
    "- NLP can be used for \n",
    "    1. Review if the text is good or bad.\n",
    "    1. Predict the Categories and genere of article & Books\n",
    "    1. Speach recognition and translation.\n",
    "\n",
    "- Most of NLP algorithems are Classification algorithem, they include Logistic Regression, Naive Bayes, CART (Decision tree, Markov models).\n",
    "\n",
    "- A very well-known model in NLP is **Bag of Words**.\n",
    "    - a model used to preprocess the texts to classify before fitting the classification algorithms on the observations containing the texts.\n",
    "\n",
    "### Classical vs Deep Learning Models\n",
    "<img src='./nlp_photos/types_nlp.png' height='200px'>\n",
    "\n",
    "**Examaples**\n",
    "1. Clasiscal NLP (green)\n",
    "    - If-Else Rules (Chatbot)\n",
    "    - Audio frequency components analysis (Speech Recognition)\n",
    "    - Bag-of-words model (Classification)\n",
    "\n",
    "1. DNLP (purple)\n",
    "    - CNN for text Recognition (Classification)\n",
    "\n",
    "## Bag of Words\n",
    "- We cannot directly feed our text into that algorithm. Hence, Bag of Words model is used to preprocess the text\n",
    "\n",
    "- It is achived by converting text into a bag of words, which keeps a count of the total occurrences of most frequently used words. and stores it in to an array at corresponding indexes.\n",
    "\n",
    "<img src='https://aiml.com/wp-content/uploads/2023/02/disadvantage-bow-1024x650.png' height='400px'>\n",
    "\n",
    "- We feed the Arrays to the algoritem, the pattern derived is associated to specific results. \n",
    "\n",
    "## Implememting NLP\n",
    "### 1. Sentimental analysis\n",
    "\n",
    "### pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "# quoting = 3 to ignore double quotes\n",
    "# tsv is a tab separated value, delimiter = '\\t',   default is ','\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t few sentences in corpus \n",
      "\n",
      "wow love place\n",
      "crust not good\n",
      "not tasti textur nasti\n",
      "stop late may bank holiday rick steve recommend love\n",
      "select menu great price\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords') # stopwords {a, an, the} have no significance in results. (non relevent words)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Simplyfies words for root meaning, eg.\"Loved\" -> \"love\",  \"wonderful\" -> \"wonder\"\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.remove('not')\n",
    "\n",
    "# Cleaning the text\n",
    "corpus = []\n",
    "for i in range(0, dataset.shape[0]): # length of dataset\n",
    "    \n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) # replace all non-alphabets with spaces in Review column of dataset\n",
    "    review = review.lower() # convert to lower case\n",
    "    review = review.split() # split into individual words\n",
    "    \n",
    "    # remove stopwords and Applying ps.stem(word) --> stemming to every word in a sentence.\n",
    "    review = [ps.stem(word) for word in review if not word in all_stopwords]\n",
    "    review = ' '.join(review) # join words with Spaces in between to form a sentence\n",
    "    corpus.append(review)\n",
    "\n",
    "print('\\n \\t few sentences in corpus \\n')\n",
    "for i in range(0, 5):\n",
    "    print(corpus[i])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Bag-of-words model\n",
    "\n",
    "- the CountVectorizer() object accept only one parameter. **max_features = 1000**, it is 1000 most frequent words, to minimize to dimensionality\n",
    "\n",
    "- focus is only on words that are frequently used, or have significace, names and numbers are ignored.\n",
    "\n",
    "**Note :**<br>\n",
    "- For first time, Use the object without and parameters(max_features). it will help you find total number of unique words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Finding total number of words\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(len(X[0])) # length of First row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see that the dataset has 1567 unique words, \n",
    "# so we can use 1500 features in our model\n",
    "\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "X = cv.fit_transform(corpus).toarray() # Independent variable\n",
    "y = dataset.iloc[:, -1].values # Depentent variable\n",
    "\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Naive Bayes model\n",
    "\n",
    "- Can use any Classifier, to catogorize the corpus into 'Yes' & 'No' groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 42]\n",
      " [12 91]]\n",
      "\t Accuracy is 0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print('\\t Accuracy is', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Single (Custom) reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_review= ['the temperature inside is Hot, AC not working',\n",
    "            'The food is not good.',\n",
    "            'I liked the pasta.',\n",
    "            \"the staff was very friendly, it was good\",\n",
    "            \"Nice place to visit\",\n",
    "            \"I hate the food\"]\n",
    "\n",
    "new_corpus = []\n",
    "for review in new_review:\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "    review = ' '.join(review)\n",
    "    new_corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperatur insid hot ac not work, --> Positive\n",
      "food not good, --> Positive\n",
      "like pasta, --> Positive\n",
      "staff friendli good, --> Positive\n",
      "nice place visit, --> Positive\n",
      "hate food, --> Negative\n"
     ]
    }
   ],
   "source": [
    "results = classifier.predict(cv.transform(new_corpus).toarray())\n",
    "for i in range(0, len(new_corpus)):\n",
    "    if results[i] == 1:\n",
    "        print(f\"{new_corpus[i]}, --> Positive\")\n",
    "    else:\n",
    "        print(f\"{new_corpus[i]}, --> Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Kernal SVM (RBF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89  8]\n",
      " [36 67]]\n",
      " Accuracy is 0.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Using SVC methord from Scikit learn library\n",
    "classifier2 = SVC(kernel=\"rbf\", random_state=0)\n",
    "classifier2.fit(X_train,y_train)\n",
    "\n",
    "# Predicting results\n",
    "y_pred2 = classifier2.predict(X_test)\n",
    "\n",
    "# Creating the Confusion matrix\n",
    "cm2 = confusion_matrix(y_test,y_pred2)\n",
    "print(cm2)\n",
    "print(f' Accuracy is {accuracy_score(y_pred2,y_test)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperatur insid hot ac not work, --> Negative\n",
      "food not good, --> Negative\n",
      "like pasta, --> Negative\n",
      "staff friendli good, --> Positive\n",
      "nice place visit, --> Positive\n",
      "hate food, --> Negative\n"
     ]
    }
   ],
   "source": [
    "results = classifier2.predict(cv.transform(new_corpus).toarray())\n",
    "for i in range(0, len(new_corpus)):\n",
    "    if results[i] == 1:\n",
    "        print(f\"{new_corpus[i]}, --> Positive\")\n",
    "    else:\n",
    "        print(f\"{new_corpus[i]}, --> Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seem like the SVM (non-linear kernal) performed Better, \n",
    "    - The Accuraccy may be higher for SVM but the Type 2 error is also very high, which may results in more incorrect negative, as it is the case in the example.\n",
    "\n",
    "- As for the Naive bayes Model it Performed Bad, the Type 1 error is more with lower Accuracy, resulting in More incorrect positive responces.\n",
    "\n",
    "- Fine tunning the Regular Expression, improving the Cleannig methords, and Choosing the Optimal Classification Algoritem can help in Improve the Accuracy and Prediction Quality. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
