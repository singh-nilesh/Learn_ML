{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkout \n",
    "<a href=\"./maths_for_ml.md\"> Model selection and statistics for ML</a>\n",
    "# Multiple Linear Regression\n",
    "### A statistical technique that uses two or more independent variables to predict the outcome of a dependent variable.\n",
    "\n",
    "## $$ y = b_0 + b_1X_1 + b_2X_2 + b_3X_3 + ... + b_8D_1 + b_9D_2 $$\n",
    "![alt text](https://blogs.sas.com/content/iml/files/2018/01/reglabels1.png)\n",
    "\n",
    "here, \n",
    "- **Y** is Dependent variable\n",
    "- **b<sub>0</sub>** is y-intercept(constant)\n",
    "- **b<sub>1</sub>** & **X<sub>1</sub>** is Slope coefficent & observations for first Independent variable\n",
    "- **b<sub>2</sub>** & **X<sub>2</sub>** is Slope coefficent & observations for other Independent variable\n",
    "- **b<sub>8</sub>** is Slope coefficent for Dummy variables\n",
    "- **D<sub>1</sub>** is Dummy variable observations.\n",
    "\n",
    "### Note:\n",
    "- Dummy variables are catogorical dependent variables (consits of letters) that have been converted to numeric from through encoding.\n",
    "- always include **(n-1)** number of Dummy variables form a single set. That is If there are 2 columns of Dummy variable, include only 1. For 9 Dummy variables include (8-1)=7 columns.\n",
    "- For a diffrent set of Dummy variable include another (n-1) set of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./reg_dataset/50_Startups.csv\")\n",
    "\n",
    "X = dataset.iloc[:, :-1].values \n",
    "Y = dataset.iloc[:, -1].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[165349.2 136897.8 471784.1 'New York']  --> 192261.83\n",
      "[162597.7 151377.59 443898.53 'California']  --> 191792.06\n",
      "[153441.51 101145.55 407934.54 'Florida']  --> 191050.39\n",
      "[144372.41 118671.85 383199.62 'New York']  --> 182901.99\n",
      "[142107.34 91391.77 366168.42 'Florida']  --> 166187.94\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'{X[i]}  --> {Y[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the categorical data(city) using OneHotEncoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[-1])], remainder='passthrough') \n",
    "# [-1] is column index with categorical data.\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0 0.0 1.0 165349.2 136897.8 471784.1]\n",
      "[1.0 0.0 0.0 162597.7 151377.59 443898.53]\n",
      "[0.0 1.0 0.0 153441.51 101145.55 407934.54]\n",
      "[0.0 0.0 1.0 144372.41 118671.85 383199.62]\n",
      "[0.0 1.0 0.0 142107.34 91391.77 366168.42]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'{X[i]}')\n",
    "# first 3 columns is the column 'city' Encoded in 0 & 1, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "In Multiple linear regression, there is no need to apply feature scalling as, the coefficent **b<sub>1</sub>** in  **b<sub>1</sub>X<sub>1</sub>** compensates for the Extream values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the dataset into Training set and Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the dataset & Predicting the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103015.202  for  103282.38 \t diff ---> 267.178\n",
      "132582.278  for  144259.4 \t diff ---> 11677.122\n",
      "132447.738  for  146121.95 \t diff ---> 13674.212\n",
      "71976.099  for  77798.83 \t diff ---> 5822.731\n",
      "178537.482  for  191050.39 \t diff ---> 12512.908\n",
      "116161.242  for  105008.31 \t diff ---> 11152.932\n",
      "67851.692  for  81229.06 \t diff ---> 13377.368\n",
      "98791.734  for  97483.56 \t diff ---> 1308.174\n",
      "113969.435  for  110352.25 \t diff ---> 3617.185\n",
      "167921.066  for  166187.94 \t diff ---> 1733.126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x_train,y_train)\n",
    "results = regressor.predict(x_test)\n",
    "for i in range(10):\n",
    "    diff = round(abs(results[i] - y_test[i]),3)  \n",
    "    print(f'{round(results[i],3)}  for  {y_test[i]} \\t diff ---> {diff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualising the dataset\n",
    "we can't plot a multiple regression on a plain. \n",
    "<br>Hear is some complex numpy methods for visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103015.2  103282.38]\n",
      " [132582.28 144259.4 ]\n",
      " [132447.74 146121.95]\n",
      " [ 71976.1   77798.83]\n",
      " [178537.48 191050.39]\n",
      " [116161.24 105008.31]\n",
      " [ 67851.69  81229.06]\n",
      " [ 98791.73  97483.56]\n",
      " [113969.44 110352.25]\n",
      " [167921.07 166187.94]]\n"
     ]
    }
   ],
   "source": [
    "# this methord tranforms the matrix(vertical(1) <--> horzontal(0)) and concatinates\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((results.reshape(len(results), 1), y_test.reshape(len(y_test), 1)), axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "- sklearn.linear_model import LinearRegression, handels the Model selction automatical.\n",
    "- if using other libraries or languages (' R '), one has to take into consideration the the significance level and P-values. <br> and have to decide manualy which model to use.\n",
    "- It is Optional, can learn <a href='./backwards_elimination.ipynb'>'Backwards elimination'</a> here, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
