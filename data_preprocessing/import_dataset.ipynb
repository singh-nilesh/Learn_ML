{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### this is a Demo file to learn how to import a dataset in  *CSV* format for ML ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries imported\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "print(\"libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "y =  ['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "x = dataset.iloc[: , :-1].values  #indexes all the rows ':' & all columns except last':-1'\n",
    "y = dataset.iloc[:,-1].values #indexes for all rows of last column\n",
    "\n",
    "print(x)\n",
    "print('y = ',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### removing empty cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# replacing the null values.\n",
    "imputer.fit(x[:,1:3])\n",
    "x[:,1:3] = imputer.transform(x[:,1:3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data\n",
    "- the process of encoding categorical data(usually names) into numerical data for individual categories.\n",
    "- numerical data is more suitable for Machine learning training models.\n",
    "- It is done so that the machine can identify the possible re-occurring name(A-Z chars.) and can be classified into categories.\n",
    "\n",
    "*One Hot Encoding*\n",
    "- used for datasets containing **more than 2** unique values.\n",
    "- a technique that we use to represent categorical variables as numerical values in a machine-learning model.\n",
    "- the categorical data is divided into multiple columns of 1's and 0's where unique combinations in rows represent the unique category.\n",
    "- Non encoded data can lead to increased dimensionality, as a separate column is created for each category in the variable.\n",
    "- make the model more complex and slow to train.\n",
    "\n",
    "### Encoding the Independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#object                 |transformers=(operation, Methord of OP, columns)|     |remainder= columns not to be transformed|\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])],   remainder='passthrough')\n",
    "\n",
    "# fit_transform dosen't return an array but our ML model needs an array to train.\n",
    "x = np.array(ct.fit_transform(x))\n",
    "\n",
    "print(x)\n",
    "# here the country names(in char) has been transformed to 3 column repesented by  combiantions of 0 & 1(float)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Dependent variables\n",
    "*Label Encoding*\n",
    "- Used for dataset containg **2 category** that can be represented as 0 or 1.\n",
    "-  process of encoding categorical data(ususally names) into numerical data for individual category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(y)\n",
    "\n",
    "# here (yes/no) from 'y' has been transformed to 0's & 1's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into Training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test =>\n",
      " [[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n",
      "y_test =>\n",
      " [0 1]\n",
      "x_train =>\n",
      " [[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n",
      "y_train =>\n",
      " [0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#   x_train -> Independent dataset from training set\n",
    "#   x_test -> Indpendent dataset from test set\n",
    "\n",
    "#   y_train -> Dependent dataset from training set\n",
    "#   y_test -> Dependent dataset from test set\n",
    "#    \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "#                               train_test_split(independent vector, dependent vector, scale , state)\n",
    "# random_state selects random rows from the dataset\n",
    "print(\"x_test =>\\n\",x_test)\n",
    "print(\"y_test =>\\n\",y_test)\n",
    "\n",
    "print(\"x_train =>\\n\",x_train)\n",
    "print(\"y_train =>\\n\",y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "- Feature Scaling is a technique to standardize the independent features(variables) present in the data in a fixed range. \n",
    "- performed to handle highly varying magnitudes or values or units.\n",
    "\n",
    "![alt text](./feature_scaling.jpg)\n",
    "\n",
    "- standardisation works all the time, Normalisation works well with dataset that have normal distribution.\n",
    "\n",
    "*matrix of feature*\n",
    "- a term used in machine learning to describe the list of columns that contain independent variables to be processed, including all lines in the dataset. in his code, we have 2 matrix of feature, they are **x_train & x_test**\n",
    "- Feature scaling is not applied to whole dataset, It is appllied to **training set & test set** seperatly.\n",
    "- **The Scaler** is fitted only to **x_train**.\n",
    "---\n",
    "- we are **not allowed** to fit our Feature scaling on the **test set** with **traing set** because we would get **mean & standard deviation** in the features(variables values) and lead to leakege of data.\n",
    "\n",
    "- ![training and test set](./train_test_set.png)\n",
    "\n",
    "- we are **not allowed** to fit feature scaling on **dumy variables**(Categories that have been transformed using One Hot Encoding) as it wound disrupt the Encoding of data.\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38.77777777777778 52000.0]   --->   [7.45726288e-04 9.99999722e-01]\n",
      "[40.0 63777.77777777778]   --->   [6.27177577e-04 9.99999803e-01]\n",
      "[44.0 72000.0]   --->   [6.11110997e-04 9.99999813e-01]\n",
      "[38.0 61000.0]   --->   [6.22950699e-04 9.99999806e-01]\n",
      "[27.0 48000.0]   --->   [5.62499911e-04 9.99999842e-01]\n",
      "[48.0 79000.0]   --->   [6.07594825e-04 9.99999815e-01]\n",
      "[50.0 83000.0]   --->   [6.02409529e-04 9.99999819e-01]\n",
      "[35.0 58000.0]   --->   [6.03448166e-04 9.99999818e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "st = StandardScaler()\n",
    "nr = Normalizer()\n",
    "\n",
    "# fit methord calculates, Transform methord replaces.\n",
    "\n",
    "# applying Feature scaling to Independent variables only.\n",
    "\n",
    "# Feature scaling for Training set.\n",
    "# First 3 colls are Dummy variables(One Hot Encoded), excluded\n",
    "\n",
    "# object for Normalization methord ---> nr\n",
    "t_x_train = nr.fit_transform(x_train[:,3:])\n",
    "\n",
    "for i in range(len(t_x_train)):\n",
    "    print(x_train[i][3:] ,\"  --->  \", t_x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38.77777777777778 52000.0]   --->   [-0.19159184 -1.07812594]\n",
      "[40.0 63777.77777777778]   --->   [-0.01411729 -0.07013168]\n",
      "[44.0 72000.0]   --->   [0.56670851 0.63356243]\n",
      "[38.0 61000.0]   --->   [-0.30453019 -0.30786617]\n",
      "[27.0 48000.0]   --->   [-1.90180114 -1.42046362]\n",
      "[48.0 79000.0]   --->   [1.14753431 1.23265336]\n",
      "[50.0 83000.0]   --->   [1.43794721 1.57499104]\n",
      "[35.0 58000.0]   --->   [-0.74014954 -0.56461943]\n"
     ]
    }
   ],
   "source": [
    "# object for Standardization methord --> st\n",
    "t_x_train = st.fit_transform(x_train[:,3:])\n",
    "\n",
    "for i in range(len(t_x_train)):\n",
    "    print(x_train[i][3:] ,\"  --->  \", t_x_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "# feature scaling for test set.\n",
    "x_test[:,3:] = st.transform(x_test[:, 3:])\n",
    "print(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
